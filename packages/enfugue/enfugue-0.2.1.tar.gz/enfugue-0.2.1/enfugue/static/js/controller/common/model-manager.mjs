import{Controller}from"../base.mjs";import{ParentView}from"../../view/base.mjs";import{FormView}from"../../view/forms/base.mjs";import{TableView,ModelTableView}from"../../view/table.mjs";import{StringInputView,TextInputView,FloatInputView,NumberInputView,FormInputView,RepeatableInputView,SearchListInputView,ButtonInputView,SelectInputView}from"../../view/forms/input.mjs";import{isEmpty,deepClone}from"../../base/helpers.mjs";let defaultEngineSize=512;class EngineSizeInputView extends NumberInputView{static min=128;static max=2048;static step=8;static defaultValue=defaultEngineSize;static tooltip="When using chunked diffusion, this is the size of the window (in pixels) that will be encoded, decoded or inferred at once. Set the chunking size to 0 in the sidebar to disable chunked diffusion and always try to process the entire image at once."}class VAEInputView extends SelectInputView{static defaultOptions={ema:"EMA 560000",mse:"MSE 840000",xl:"SDXL",xl16:"SDXL FP16"};static placeholder="Default";static allowEmpty=!0;static tooltip="Variational Autoencoders are the model that translates images between pixel space - images that you can see - and latent space - images that the AI model understands. In general you do not need to select a particular VAE model, but you may find slight differences in sharpness of resulting images."}class SchedulerInputView extends SelectInputView{static defaultOptions={ddim:"DDIM: Denoising Diffusion Implicit Models",ddpm:"DDPM: Denoising Diffusion Probabilistic Models",deis:"DEIS: Diffusion Exponential Integrator Sampler",dpmsm:"DPM-Solver++ Multi-Step",dpmss:"DPM-Solver++ Single-Step",heun:"Heun Discrete Scheduler",dpmd:"DPM Discrete Scheduler",adpmd:"DPM Ancestral Discrete Scheduler",dpmsde:"DPM Solver SDE Scheduler",unipc:"UniPC: Predictor (UniP) and Corrector (UniC)",lmsd:"LMS: Linear Multi-Step Discrete Scheduler",pndm:"PNDM: Pseudo Numerical Methods for Diffusion Models",eds:"Euler Discrete Scheduler",eads:"Euler Ancestral Discrete Scheduler"};static tooltip="Schedulers control how an image is denoiser over the course of the inference steps. Schedulers can have small effects, such as creating 'sharper' or 'softer' images, or drastically change the way images are constructed. Experimentation is encouraged, if additional information is sought, search <strong>Diffusers Schedulers</strong> in your search engine of choice.";static placeholder="Default";static allowEmpty=!0}class MultiDiffusionSchedulerInputView extends SelectInputView{static defaultOptions={ddim:"DDIM: Denoising Diffusion Implicit Models (Recommended)",eds:"Euler Discrete Scheduler (Recommended)",ddpm:"DDPM: Denoising Diffusion Probabilistic Models (Blurrier)",eads:"Euler Ancestral Discrete Scheduler (Blurrier)",deis:"DEIS: Diffusion Exponential Integrator Sampler (Distorted)",dpmsm:"DPM-Solver++ Multi-Step (Distorted)",dpmss:"DPM-Solver++ Single-Step (Distorted)"};static tooltip="During chunked diffusion (also called multi-diffusion or sliced diffusion,) each denoising step is performed multiple times over different windows of the image. This necessitates that the scheduler be capable of stepping backward as well as forward, and not all schedulers were designed with this in mind. The schedulers in this list are supported during multi-diffusion, but only two are recommended: DDIM, which is the default scheduler for SD 1.5, and Euler Discrete, which is the default scheduler for SDXL.";static placeholder="Default";static allowEmpty=!0}class InpainterEngineSizeInputView extends EngineSizeInputView{static tooltip="This engine size functions the same as the base engine size, but only applies when inpainting.\n\n"+EngineSizeInputView.tooltip;static defaultValue=null}class RefinerEngineSizeInputView extends EngineSizeInputView{static tooltip="This engine size functions the same as the base engine size, but only applies when refining.\n\n"+EngineSizeInputView.tooltip;static defaultValue=null}class InversionInputView extends SearchListInputView{}class LoraInputView extends SearchListInputView{}class LycorisInputView extends SearchListInputView{}class CheckpointInputView extends SearchListInputView{}class LoraFormView extends FormView{static autoSubmit=!0;static fieldSets={LoRA:{model:{label:"Model",class:LoraInputView,config:{required:!0}},weight:{label:"Weight",class:FloatInputView,config:{min:0,value:1,step:.01,required:!0}}}}}class LoraFormInputView extends FormInputView{static formClass=LoraFormView}class LycorisFormView extends FormView{static autoSubmit=!0;static fieldSets={LyCORIS:{model:{label:"Model",class:LycorisInputView,config:{required:!0}},weight:{label:"Weight",class:FloatInputView,config:{min:0,value:1,step:.01,required:!0}}}}}class LycorisFormInputView extends FormInputView{static formClass=LycorisFormView}class MultiLoraInputView extends RepeatableInputView{static memberClass=LoraFormInputView}class MultiLycorisInputView extends RepeatableInputView{static memberClass=LycorisFormInputView}class MultiInversionInputView extends RepeatableInputView{static memberClass=InversionInputView}class ModelForm extends FormView{static canCancel=!0;static fieldSets={Name:{name:{class:StringInputView,label:"Name",config:{required:!0,tooltip:"Give your model a name that describes what you want it to do - for example, if you're using a photorealistic model and use phrases related to central framing, bokeh focus and and saturated colors, you could call this configuration &ldquo;Product Photography.%rdquo;"}}},Model:{checkpoint:{class:CheckpointInputView,label:"Checkpoint",config:{required:!0,tooltip:"A &ldquo;checkpoint&rdquo; represents the state of the Stable Diffusion model at a given point in it's training. Generally, checkpoints are started from a particular version of the foundation Stable Diffusion model (1.5, 2.1, XL 1.0, etc.) and fine-tuned on a particular style or subject of imagery, though you can also use the foundation checkpoints on their own."}}},"Adaptations and Modifications":{lora:{class:MultiLoraInputView,label:"LoRA",config:{tooltip:"LoRA stands for <strong>Low Rank Adapation</strong>, it is a kind of fine-tuning that can perform very specific modifications to Stable Diffusion such as training an individual's appearance, new products that are not in Stable Diffusion's training set, etc."}},lycoris:{class:MultiLycorisInputView,label:"LyCORIS",config:{tooltip:"LyCORIS stands for <strong>LoRA beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion</strong>, a novel means of performing low-rank adaptation introduced in early 2023."}},inversion:{class:MultiInversionInputView,label:"Textual Inversions",config:{tooltip:"Textual Inversion is another kind of fine-tuning that teaches novel concepts to Stable Diffusion in a small number of images, which can be used to positively or negatively affect the impact of various prompts."}}},"Additional Models":{vae:{class:VAEInputView,label:"VAE"},refiner:{class:CheckpointInputView,label:"Refining Checkpoint",config:{tooltip:"Refiner checkpoints were introduced with SDXL 0.9 - these are checkpoints specifically trained to improve detail, shapes, and generally improve the quality of images generated from the base model. These are optional, and do not need to be specifically-trained refinement checkpoints - you can try mixing and matching checkpoints for different styles, though you may wish to ensure the related checkpoints were trained on the same size images."}},inpainter:{class:CheckpointInputView,label:"Inpainting Checkpoint",config:{tooltip:"An inpainting checkpoint if much like a regular Stable Diffusion checkpoint, but it additionally includes the ability to input which parts of the image can be changed and which cannot. This is used when you specifically request an image be inpainted, but is also used in many other situations in Enfugue; such as when you place an image on the canvas that doesn't cover the entire space, or use an image that has transparency in it (either before or after removing it's background.) When you don't select an inpainting checkpoint and request an inpainting operation, one will be created dynamically from the main checkpoint at runtime."}}},Engine:{size:{class:EngineSizeInputView,label:"Size",config:{required:!0}},refiner_size:{class:RefinerEngineSizeInputView,label:"Refiner Size"},inpainter_size:{class:InpainterEngineSizeInputView,label:"Inpainter Size"}},Prompts:{prompt:{class:TextInputView,label:"Prompt",tooltip:"This prompt will be appended to every prompt you make when using this model. Use this field to add trigger words, style or quality phrases that you always want to be included."},negative_prompt:{class:TextInputView,label:"Negative Prompt",tooltip:"This prompt will be appended to every negative prompt you make when using this model. Use this field to add trigger words, style or quality phrases that you always want to be excluded."}},"Additional Defaults":{scheduler:{class:SchedulerInputView,label:"Scheduler"},multi_scheduler:{class:MultiDiffusionSchedulerInputView,label:"Multi-Diffusion Scheduler"},width:{label:"Width",class:NumberInputView,config:{tooltip:"The width of the canvas in pixels.",min:128,max:16384,step:8,value:null}},height:{label:"Height",class:NumberInputView,config:{tooltip:"The height of the canvas in pixels.",min:128,max:16384,step:8,value:null}},chunking_size:{label:"Chunk Size",class:NumberInputView,config:{tooltip:"<p>The number of pixels to move the frame when doing chunked diffusion.</p><p>When this number is greater than 0, the engine will only ever process a square in the size of the configured model size at once. After each square, the frame will be moved by this many pixels along either the horizontal or vertical axis, and then the image is re-diffused. When this number is 0, chunking is disabled, and the entire canvas will be diffused at once.</p><p>Disabling this (setting it to 0) can have varying visual results, but a guaranteed result is drastically increased VRAM usage for large images. A low number can produce more detailed results, but can be noisy, and takes longer to process. A high number is faster to process, but can have poor results especially along frame boundaries. The recommended value is set by default.</p>",min:0,max:2048,step:8,value:null}},chunking_blur:{label:"Chunk Blur",class:NumberInputView,config:{tooltip:"The number of pixels to feather along the edge of the frame when blending chunked diffusions together. Low numbers can produce less blurry but more noisy results, and can potentially result in visible breaks in the frame. High numbers can help blend frames, but produce blurrier results. The recommended value is set by default.",min:0,max:2048,step:8,value:null}},inference_steps:{label:"Inference Steps",class:NumberInputView,config:{tooltip:"How many steps to take during primary inference, larger values take longer to process but can produce better results.",min:0,step:1,value:null}},guidance_scale:{label:"Guidance Scale",class:NumberInputView,config:{tooltip:"How closely to follow the text prompt; high values result in high-contrast images closely adhering to your text, low values result in low-contrast images with more randomness.",min:0,max:100,step:.01,value:null}},refiner_denoising_strength:{label:"Refiner Denoising Strength",class:NumberInputView,config:{tooltip:"When using a refiner, this will control how much of the original image is kept, and how much of it is replaced with refined content. A value of 1.0 represents total destruction of the first image.",min:0,max:1,step:.01,value:null}},refiner_guidance_scale:{label:"Refiner Guidance Scale",class:NumberInputView,config:{tooltip:"When using a refiner, this will control how closely to follow the guidance of the model. Low values can result in soft details, whereas high values can result in high-contrast ones.",min:0,max:100,step:.01,value:null}},refiner_aesthetic_score:{label:"Refiner Aesthetic Score",class:NumberInputView,config:{tooltip:"Aesthetic scores are assigned to images in SDXL refinement; this controls the positive score.",min:0,max:100,step:.01,value:null}},refiner_negative_aesthetic_score:{label:"Negative Aesthetic Score",class:NumberInputView,config:{tooltip:"Aesthetic scores are assigned to images in SDXL refinement; this controls the negative score.",min:0,max:100,step:.01,value:null}}}};static collapseFieldSets=["Adaptations and Modifications","Additional Models","Additional Defaults"]}class NewModelInputView extends ButtonInputView{static defaultValue="New Model Configuration";static className="new-model-input-view"}class ModelManagerController extends Controller{static modelWindowWidth=400;static modelWindowHeight=1e3;static managerWindowWidth=800;static managerWindowHeight=600;async createManager(){this.tableView=new ModelTableView(this.config,this.model.DiffusionModel),this.buttonView=new NewModelInputView(this.config),this.tableView.setColumns({name:"Name",model:"Model",size:"Size",prompt:"Prompt",negative_prompt:"Negative Prompt"}),this.tableView.setFormatter("size",(e=>`${e}px`)),this.tableView.addButton("Edit","fa-solid fa-edit",(async e=>{let i=e.getAttributes();if(i.checkpoint=i.model,i.lora=isEmpty(e.lora)?[]:e.lora.map((e=>e.getAttributes())),i.lycoris=isEmpty(e.lycoris)?[]:e.lycoris.map((e=>e.getAttributes())),i.inversion=isEmpty(e.inversion)?[]:e.inversion.map((e=>e.model)),i.vae=isEmpty(e.vae)?null:e.vae[0].name,isEmpty(e.refiner)||(i.refiner=e.refiner[0].model,i.refiner_size=e.refiner[0].size),isEmpty(e.inpainter)||(i.inpainter=e.inpainter[0].model,i.inpainter_size=e.inpainter[0].size),!isEmpty(e.config))for(let t of e.config)i[t.configuration_key]=t.configuration_value;if(!isEmpty(e.scheduler))for(let t of e.scheduler)"multi_diffusion"===t.context?i.multi_scheduler=t.name:i.scheduler=t.name;let t,n=new ModelForm(this.config,deepClone(i));n.onChange((async e=>{isEmpty(n.values.refiner)?n.removeClass("show-refiner"):n.addClass("show-refiner"),isEmpty(n.values.inpainter)?n.removeClass("show-inpainter"):n.addClass("show-inpainter")})),n.onSubmit((async i=>{try{await this.model.patch(`/models/${e.name}`,null,null,i),isEmpty(t)||t.remove(),this.tableView.requery()}catch(e){let i=isEmpty(e)?"Couldn't communicate with server.":isEmpty(e.detail)?`${e}`:e.detail;this.notify("error","Couldn't update model",i),n.enable()}})),n.onCancel((()=>t.remove())),t=await this.spawnWindow(`Edit ${e.name}`,n,this.constructor.modelWindowWidth,this.constructor.modelWindowHeight)})),this.tableView.addButton("Delete","fa-solid fa-trash",(async e=>{try{await this.model.delete(`/models/${e.name}`),this.tableView.requery()}catch(e){let i=isEmpty(e)?"Couldn't communicate with server.":isEmpty(e.detail)?`${e}`:e.detail;this.notify("error","Couldn't delete model",i),modelForm.enable()}})),this.buttonView.onChange((()=>{this.showNewModel()}));let e=new ParentView(this.config);return e.addChild(this.tableView),e.addChild(this.buttonView),e}async createModelForm(){let e=new ModelForm(this.config);return e.onSubmit((async i=>{try{await this.model.post("/models",null,null,i);isEmpty(this.newModelWindow)||(this.newModelWindow.remove(),this.newModelWindow=null),isEmpty(this.managerWindow)||isEmpty(this.tableView)||this.tableView.requery()}catch(i){let t=isEmpty(i)?"Couldn't communicate with server.":isEmpty(i.detail)?`${i}`:i.detail;this.notify("Error","Couldn't create model",t),e.enable()}})),e.onCancel((()=>{this.newModelWindow.remove(),this.newModelWindow=null})),e}async showNewModel(){isEmpty(this.newModelWindow)?(this.newModelWindow=await this.spawnWindow("New Configuration",this.createModelForm(),this.constructor.modelWindowWidth,this.constructor.modelWindowHeight),this.newModelWindow.onClose((()=>{delete this.newModelWindow}))):this.newModelWindow.focus()}async showManager(){isEmpty(this.managerWindow)?(this.managerWindow=await this.spawnWindow("Model Configuration Manager",this.createManager(),this.constructor.managerWindowWidth,this.constructor.managerWindowHeight),this.managerWindow.onClose((()=>{delete this.managerWindow}))):this.managerWindow.focus()}async initialize(){defaultEngineSize=this.application.config.model.invocation.defaultEngineSize,LoraInputView.defaultOptions=async()=>this.model.get("/lora"),LycorisInputView.defaultOptions=async()=>this.model.get("/lycoris"),CheckpointInputView.defaultOptions=async()=>this.model.get("/checkpoints"),InversionInputView.defaultOptions=async()=>this.model.get("/inversions")}}export{ModelManagerController,CheckpointInputView,MultiLoraInputView,MultiLycorisInputView,MultiInversionInputView,EngineSizeInputView,RefinerEngineSizeInputView,InpainterEngineSizeInputView,VAEInputView,SchedulerInputView,MultiDiffusionSchedulerInputView};
